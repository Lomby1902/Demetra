{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DeJx7tguCjnb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giovanni/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, PredictionErrorDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from tabulate import tabulate\n",
    "from sklearn_genetic import GASearchCV\n",
    "from sklearn_genetic.space import Categorical, Integer, Continuous\n",
    "\n",
    "path = 'SAMPLE_DATA_SET.xlsx'\n",
    "\n",
    "# Read and load dataset\n",
    "df= pd.read_excel(path, sheet_name=[0, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LoGSDzBCkPR"
   },
   "source": [
    "INPUT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1714748289388,
     "user": {
      "displayName": "Giovanni Lombardo",
      "userId": "17752443154386048654"
     },
     "user_tz": -120
    },
    "id": "JJcSpwV0CmiK",
    "outputId": "f0820786-ee5f-4976-8605-6fe7d498a257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(839, 444)\n"
     ]
    }
   ],
   "source": [
    "X =df.get(0)\n",
    "X = (X.iloc[:,1:]).values\n",
    "X = X.T\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wMhRqrsCojw"
   },
   "source": [
    "OUTPUT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1714748289388,
     "user": {
      "displayName": "Giovanni Lombardo",
      "userId": "17752443154386048654"
     },
     "user_tz": -120
    },
    "id": "QDo7dWOmCrT9",
    "outputId": "0ca1733b-ea5c-459f-b4c6-ecfda32aa195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(839, 2)\n"
     ]
    }
   ],
   "source": [
    "Y =df.get(1)\n",
    "Y = (Y.iloc[:,:]).values\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlPjOdAQKgqr"
   },
   "source": [
    "RIDGE REGRESSION with closed form solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868
    },
    "executionInfo": {
     "elapsed": 4185248,
     "status": "error",
     "timestamp": 1714752830985,
     "user": {
      "displayName": "Giovanni Lombardo",
      "userId": "17752443154386048654"
     },
     "user_tz": -120
    },
    "id": "Y7HQm0eyKWEH",
    "outputId": "5384448a-0c74-4df3-b631-402f97ec8602"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giovanni/.local/lib/python3.10/site-packages/sklearn_genetic/genetic_search.py:279: UserWarning: Warning, only one parameter was provided to the param_grid, the optimization routine might not have effect or it could lead to errors, it's advised to use at least 2 parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
      "0  \t10    \t-2.0755\t0.0418157  \t-2.05335   \t-2.19696   \n",
      "1  \t20    \t-2.0534\t0.000100787\t-2.05335   \t-2.0537    \n",
      "2  \t20    \t-2.05344\t0.000221908\t-2.05335   \t-2.0541    \n",
      "3  \t20    \t-2.05345\t0.00021957 \t-2.05335   \t-2.0541    \n",
      "4  \t20    \t-2.05336\t1.07786e-05\t-2.05335   \t-2.05338   \n",
      "5  \t20    \t-2.05337\t1.18946e-05\t-2.05335   \t-2.05338   \n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/giovanni/.local/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 426, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 108, in get\n    if not self._rlock.acquire(block, timeout):\nKeyboardInterrupt\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_133602/3016024934.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m                               generations=10)\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mevolved_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Make predictions using the testing set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn_genetic/genetic_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, callbacks)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# Optimization routine from the selected evolutionary algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;31m# Update the _n_iterations value as the algorithm could stop earlier due a callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn_genetic/genetic_search.py\u001b[0m in \u001b[0;36m_select_algorithm\u001b[0;34m(self, pop, stats, hof)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mselected_algorithm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithms_factory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mselected_algorithm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m             pop, log, gen = selected_algorithm(\n\u001b[0m\u001b[1;32m    573\u001b[0m                 \u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn_genetic/algorithms.py\u001b[0m in \u001b[0;36meaMuPlusLambda\u001b[0;34m(population, toolbox, mu, lambda_, cxpb, mutpb, ngen, stats, halloffame, callbacks, verbose, estimator, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0minvalid_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moffspring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mfitnesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitnesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn_genetic/genetic_search.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, individual)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;31m# Compute the cv-metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    400\u001b[0m             \u001b[0mlocal_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    431\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    432\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         )\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1752\u001b[0m             \u001b[0;31m# worker traceback.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1754\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1755\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         \u001b[0;31m# called directly or if the generator is gc'ed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_job\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m             \u001b[0merror_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_warn_exit_early\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0;31m# callback thread, and is stored internally. It's just waiting to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m             \u001b[0;31m# be returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_or_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;31m# For other backends, the main thread needs to run the retrieval step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_ERROR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Ridge cholesky Parameter 1\")\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "\n",
    "#number of trials\n",
    "trials = 100\n",
    "np.random.seed(42)\n",
    "seeds = np.random.randint(trials, size=trials)\n",
    "\n",
    "ripartisions = [i/100 for i in range(5,55,5)]\n",
    "results = []  #[\"Ripartition\", \"Best_MSE1\", \"Best_MSE2\",\"MSE1_mean\",\"MSE2_mean\",\"MSE1_var\",\"MSE2_var\",\"alpha1\",\"alpha2\"]\n",
    "\n",
    "print(\"Param 1\")\n",
    "for ripartition in ripartisions:\n",
    "\n",
    "    '''PARAM 1'''\n",
    "\n",
    "    MSE_param1 = []\n",
    "\n",
    "    best_MSE_1 = 100000000\n",
    "    best_data_1 = {\"Y_test\":None, \"Y_pred\":None,\"alpha\":None}\n",
    "\n",
    "    run_name = str(n_components) + \" components \" + str(ripartition) + \" ripartition\"\n",
    "    with mlflow.start_run(run_name=name_experiment):\n",
    "    i = 0\n",
    "    for i in range(trials):\n",
    "        #Ripartition in training and test and\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y[:,0], test_size=ripartition, random_state=seeds[i])\n",
    "        param_grid = {'alpha': Continuous(1/100000,0.01)}\n",
    "        ridge_reg_cl = Ridge(solver=\"cholesky\")\n",
    "        \n",
    "        evolved_estimator = GASearchCV(estimator=ridge_reg_cl,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_squared_error',\n",
    "                              param_grid=param_grid,\n",
    "                              n_jobs=-1,\n",
    "                              verbose=True,\n",
    "                              population_size=10,\n",
    "                              generations=10)\n",
    "\n",
    "        evolved_estimator.fit(X_train, Y_train)\n",
    "\n",
    "        # Make predictions using the testing set\n",
    "        Y_pred_ridge_cl =  evolved_estimator.predict(X_test)\n",
    "\n",
    "        #save MSE of the first parameters\n",
    "        MSE_param1.append(mean_squared_error(Y_test, Y_pred_ridge_cl))\n",
    "\n",
    "        #check if this is the best model in terms of MSE\n",
    "        if MSE_param1[-1] < best_MSE_1:\n",
    "          best_MSE_1 = MSE_param1[-1]\n",
    "          best_data_1[\"Y_test\"] = Y_test\n",
    "          best_data_1[\"Y_pred\"] = Y_pred_ridge_cl\n",
    "          best_data_1[\"alpha\"] =  float(evolved_estimator.best_params_[\"alpha\"])\n",
    "\n",
    "mlflow.set_experiment(\"Ridge cholesky Parameter 2\")\n",
    "\n",
    "    '''PARAM 2'''\n",
    "    MSE_param2 = []\n",
    "\n",
    "    best_MSE_2 = 100000000\n",
    "    best_data_2 = {\"Y_test\":None, \"Y_pred\":None,\"alpha\":None}\n",
    "\n",
    "    i = 0\n",
    "    for i in range(trials):\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y[:,1], test_size=ripartition, random_state=seeds[i])\n",
    "        param_grid = param_grid = {'alpha': Continuous(1/100000,0.01)}\n",
    "        ridge_reg_cl = Ridge(solver=\"cholesky\")\n",
    "\n",
    "        evolved_estimator = GASearchCV(estimator=ridge_reg_cl,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_squared_error',\n",
    "                              param_grid=param_grid,\n",
    "                              n_jobs=-1,\n",
    "                              verbose=True,\n",
    "                              population_size=10,\n",
    "                              generations=10)\n",
    "\n",
    "        evolved_estimator.fit(X_train, Y_train)\n",
    "\n",
    "        # Make predictions using the testing set\n",
    "        Y_pred_ridge_cl =  evolved_estimator.predict(X_test)\n",
    "        #save MSE of the second parameters\n",
    "        MSE_param2.append(mean_squared_error(Y_test, Y_pred_ridge_cl))\n",
    "\n",
    "        #check if this is the best model in terms of MSE\n",
    "        if MSE_param2[-1] < best_MSE_2:\n",
    "          best_MSE_2 = MSE_param2[-1]\n",
    "          best_data_2[\"Y_test\"] = Y_test\n",
    "          best_data_2[\"Y_pred\"] = Y_pred_ridge_cl\n",
    "          best_data_2[\"alpha\"] =  float(evolved_estimator.best_params_[\"alpha\"])\n",
    "\n",
    "\n",
    "    results.append([str(ripartition *100 ), str(best_MSE_1), str(best_MSE_2),\n",
    "                    (np.mean(MSE_param1)),str(np.mean(MSE_param2)), str(np.var(MSE_param1)),\n",
    "                    str(np.var(MSE_param2)), str(np.var(MSE_param2)), str(best_data_1[\"alpha\"]), str(best_data_2[\"alpha\"])])\n",
    "\n",
    "    '''PLOT THE BEST MODELS'''\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    PredictionErrorDisplay.from_predictions(y_true=best_data_1[\"Y_test\"], y_pred=best_data_1[\"Y_pred\"], kind = \"actual_vs_predicted\", ax=axs[0])\n",
    "    axs[0].set_title(\"Parameter 1\"+\"\\n\"+ \"MSE: \" +  str(min(MSE_param1)) + \"\\n\" + \" alpha= \"+ str(best_data_1[\"alpha\"]))\n",
    "    disp2= PredictionErrorDisplay.from_predictions(y_true=best_data_2[\"Y_test\"], y_pred=best_data_2[\"Y_pred\"], kind = \"actual_vs_predicted\", ax=axs[1])\n",
    "    axs[1].set_title(\"Parameter 2\"+\"\\n\"+ \"MSE: \" +  str(min(MSE_param2)) + \"\\n\" + \" alpha= \"+ str(best_data_2[\"alpha\"]))\n",
    "    fig.suptitle(\"Ridge Regression with closed form\"+\"\\n\"+ \"Best MSE results\" + \"\\n\"+ f\"Test size(%): {ripartition * 100}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "'''SHOW RESULTS'''\n",
    "headers = [\"Test size(%)\",\"Best(MSE1)\", \"Best(MSE2)\",\"Mean(MSE1)\", \"Mean(MSE2)\", \"Var(MSE1)\", \"Var(MSE2)\",\"alpha1\",\"alpha2\"]\n",
    "\n",
    "print(tabulate(results, headers,  tablefmt=\"double_outline\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDlfwPf7Kpmz"
   },
   "source": [
    "**RIDGE REGRESSION with SGD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3quqTzaKwN6"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "#number of trials\n",
    "trials = 100\n",
    "np.random.seed(42)\n",
    "seeds = np.random.randint(trials, size=trials)\n",
    "\n",
    "ripartisions = [i/100 for i in range(5,55,5)]\n",
    "results = []  #[\"Ripartition\", \"Best_MSE1\", \"Best_MSE2\",\"MSE1_mean\",\"MSE2_mean\",\"MSE1_var\",\"MSE2_var\",\"alpha1\",\"alpha2\"]\n",
    "\n",
    "\n",
    "for ripartition in ripartisions:\n",
    "\n",
    "    '''PARAM 1'''\n",
    "\n",
    "    MSE_param1 = []\n",
    "\n",
    "    best_MSE_1 = 100000000\n",
    "    best_data_1 = {\"Y_test\":None, \"Y_pred\":None,\"alpha\":None}\n",
    "\n",
    "    i = 0\n",
    "    for i in range(trials):\n",
    "        #Ripartition in training and test and\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y[:,0], test_size=ripartition, random_state=seeds[i])\n",
    "        param_grid = {'alpha': Continuous(1/100000,0.01)}\n",
    "        ridge_reg_sgd = SGDRegressor(max_iter=5000, tol=1e-5, penalty=\"l2\", eta0=0.01,\n",
    "                                            n_iter_no_change=10, random_state=42)\n",
    "\n",
    "        evolved_estimator = GASearchCV(estimator=ridge_reg_cl,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_squared_error',\n",
    "                              param_grid=param_grid,\n",
    "                              n_jobs=-1,\n",
    "                              verbose=True,\n",
    "                              population_size=10,\n",
    "                              generations=10)\n",
    "\n",
    "        evolved_estimator.fit(X_train, Y_train)\n",
    "\n",
    "        # Make predictions using the testing set\n",
    "        Y_pred_ridge_sgd =  evolved_estimator.predict(X_test)\n",
    "\n",
    "        #save MSE of the first parameters\n",
    "        MSE_param1.append(mean_squared_error(Y_test, Y_pred_ridge_sgd))\n",
    "\n",
    "        #check if this is the best model in terms of MSE\n",
    "        if MSE_param1[-1] < best_MSE_1:\n",
    "          best_MSE_1 = MSE_param1[-1]\n",
    "          best_data_1[\"Y_test\"] = Y_test\n",
    "          best_data_1[\"Y_pred\"] = Y_pred_ridge_sgd\n",
    "          best_data_1[\"alpha\"] =  float(evolved_estimator.best_params_[\"alpha\"])\n",
    "\n",
    "\n",
    "    '''PARAM 2'''\n",
    "    MSE_param2 = []\n",
    "\n",
    "    best_MSE_2 = 100000000\n",
    "    best_data_2 = {\"Y_test\":None, \"Y_pred\":None,\"alpha\":None}\n",
    "\n",
    "    i = 0\n",
    "    for i in range(trials):\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y[:,1], test_size=ripartition, random_state=seeds[i])\n",
    "        param_grid = {'alpha': Continuous(1/100000,0.01)}\n",
    "        ridge_reg_sgd = SGDRegressor(max_iter=5000, tol=1e-5, penalty=\"l2\", eta0=0.01,\n",
    "                                            n_iter_no_change=10, random_state=42)\n",
    "\n",
    "        evolved_estimator = GASearchCV(estimator=ridge_reg_cl,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_squared_error',\n",
    "                              param_grid=param_grid,\n",
    "                              n_jobs=-1,\n",
    "                              verbose=True,\n",
    "                              population_size=10,\n",
    "                              generations=10)\n",
    "\n",
    "        evolved_estimator.fit(X_train, Y_train)\n",
    "\n",
    "        # Make predictions using the testing set\n",
    "        Y_pred_ridge_sgd =  evolved_estimator.predict(X_test)\n",
    "\n",
    "        #save MSE of the second parameters\n",
    "        MSE_param2.append(mean_squared_error(Y_test, Y_pred_ridge_sgd))\n",
    "\n",
    "        #check if this is the best model in terms of MSE\n",
    "        if MSE_param2[-1] < best_MSE_2:\n",
    "          best_MSE_2 = MSE_param2[-1]\n",
    "          best_data_2[\"Y_test\"] = Y_test\n",
    "          best_data_2[\"Y_pred\"] = Y_pred_ridge_sgd\n",
    "          best_data_2[\"alpha\"] =  float(evolved_estimator.best_params_[\"alpha\"])\n",
    "\n",
    "    results.append([str(ripartition *100 ), str(best_MSE_1), str(best_MSE_2),\n",
    "     (np.mean(MSE_param1)),str(np.mean(MSE_param2)), str(np.var(MSE_param1)), str(np.var(MSE_param2)),str(best_data_1[\"alpha\"]),str(best_data_2[\"alpha\"])])\n",
    "\n",
    "    '''PLOT THE BEST MODELS'''\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    PredictionErrorDisplay.from_predictions(y_true=best_data_1[\"Y_test\"], y_pred=best_data_1[\"Y_pred\"], kind = \"actual_vs_predicted\", ax=axs[0])\n",
    "    axs[0].set_title(\"Parameter 1\"+\"\\n\"+ \"MSE: \" +  str(min(MSE_param1)) + \"\\n\" + \" alpha= \"+ str(best_data_1[\"alpha\"]))\n",
    "    disp2= PredictionErrorDisplay.from_predictions(y_true=best_data_2[\"Y_test\"], y_pred=best_data_2[\"Y_pred\"], kind = \"actual_vs_predicted\", ax=axs[1])\n",
    "    axs[1].set_title(\"Parameter 2\"+\"\\n\"+ \"MSE: \" +  str(min(MSE_param2)) + \"\\n\" + \" alpha= \"+ str(best_data_2[\"alpha\"]))\n",
    "    fig.suptitle(\"Ridge Regression with SGD\"+\"\\n\"+ \"Best MSE results\" + \"\\n\"+ f\"Test size(%): {ripartition * 100}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "'''SHOW RESULTS'''\n",
    "headers = [\"Test size(%)\",\"Best(MSE1)\", \"Best(MSE2)\",\"Mean(MSE1)\", \"Mean(MSE2)\", \"Var(MSE1)\", \"Var(MSE2)\",\"alpha1\",\"alpha2\"]\n",
    "\n",
    "print(tabulate(results, headers,  tablefmt=\"double_outline\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNhfNv01AdXHvYyGO9QKrtm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
